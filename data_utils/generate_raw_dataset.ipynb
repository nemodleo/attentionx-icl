{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a8c668",
   "metadata": {},
   "source": [
    "## Generate raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ed0c91-47b7-4c99-b61e-92fd57933be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (15.0.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4be109bf-f9cd-4d23-b5b2-c3018c241c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def parquet_to_jsonl(jsonpath_or_parquet_pattern, jsonl_file_path, is_parquet=True, split=False, splitted_output_path=None):\n",
    "    if is_parquet:\n",
    "        # Use glob to find all files matching the pattern\n",
    "        parquet_files = glob.glob(jsonpath_or_parquet_pattern)\n",
    "\n",
    "        # Initialize an empty DataFrame\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate through each file and append to the DataFrame\n",
    "        for file in parquet_files:\n",
    "            df = pd.read_parquet(file)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    else:\n",
    "        with open(jsonpath_or_parquet_pattern) as f:\n",
    "            combined_df = pd.read_json(f, lines=True)\n",
    "\n",
    "    # Convert the combined DataFrame to JSON Lines and write to a file\n",
    "    if split:\n",
    "        df, splitted_df = train_test_split(combined_df, test_size=0.1, random_state=42)\n",
    "        df.to_json(jsonl_file_path, orient='records', lines=True)\n",
    "        splitted_df.to_json(splitted_output_path, orient='records', lines=True)\n",
    "    else:\n",
    "        combined_df.to_json(jsonl_file_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e54d4",
   "metadata": {},
   "source": [
    "`jsonpath_or_parquet_pattern`: Provide pattern of a path to the `parquet` file or a path to the `jsonl` file\n",
    "- set `is_parquet=False` if providing `jsonl` file path\n",
    "\n",
    "`jsonl_file_path`: path to the output `jsonl` file\n",
    "\n",
    "`split`: set to `True` if it needs a split (validation set doesn't exist)\n",
    "\n",
    "`splitted_output_path`: path to the splitted output `jsonl` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc94bce",
   "metadata": {},
   "source": [
    "### If using parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb091340-5089-4686-a32d-e1414494ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "jsonpath_or_parquet_pattern = '../data/yelp/train-*.parquet'  # parquet file pattern\n",
    "jsonl_file_path = 'train.jsonl'\n",
    "splitted_output_path = 'valid.jsonl'\n",
    "parquet_to_jsonl(jsonpath_or_parquet_pattern, jsonl_file_path, is_parquet=True, split=True, splitted_output_path=splitted_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41abe07d-1114-4082-87f4-21996f1b29b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "jsonpath_or_parquet_pattern = '../data/yelp/test-*.parquet' # parquet file pattern\n",
    "jsonl_file_path = 'test.jsonl'\n",
    "parquet_to_jsonl(jsonpath_or_parquet_pattern, jsonl_file_path, is_parquet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d5043",
   "metadata": {},
   "source": [
    "### If using jsonl file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4feb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "jsonpath_or_parquet_pattern = '../data/trec/trec_train.jsonl'\n",
    "jsonl_file_path = 'train.jsonl'\n",
    "splitted_output_path = 'valid.jsonl'\n",
    "parquet_to_jsonl(jsonpath_or_parquet_pattern, jsonl_file_path, is_parquet=False, split=True, splitted_output_path=splitted_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3ddf7",
   "metadata": {},
   "source": [
    "### Push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9e9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/yelp/train.jsonl\"\n",
    "val_path = \"../data/yelp/valid.jsonl\"\n",
    "test_path = \"../data/yelp/test.jsonl\"\n",
    "\n",
    "dataset_name = \"ICKD/yelp-raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "036bd3c1-ec17-4e4e-b0ae-d41f12c808b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\":train_path, \"valid\":val_path, \"test\": test_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "685f614e-4331-4d2d-92cc-396aa29c4e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 585000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 65000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaee4240-2f72-4336-b29f-eeb74b96a839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/449 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   1%|▏         | 6/449 [00:00<00:07, 59.78ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   3%|▎         | 13/449 [00:00<00:07, 61.19ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   4%|▍         | 20/449 [00:00<00:06, 61.56ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   6%|▌         | 27/449 [00:00<00:06, 61.77ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   8%|▊         | 34/449 [00:00<00:06, 61.72ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   9%|▉         | 41/449 [00:00<00:06, 61.18ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  11%|█         | 48/449 [00:00<00:06, 61.52ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  12%|█▏        | 55/449 [00:00<00:06, 61.63ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  14%|█▍        | 62/449 [00:01<00:06, 61.00ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  15%|█▌        | 69/449 [00:01<00:06, 61.08ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  17%|█▋        | 76/449 [00:01<00:06, 61.27ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  18%|█▊        | 83/449 [00:01<00:05, 61.41ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  20%|██        | 90/449 [00:01<00:05, 61.60ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  22%|██▏       | 97/449 [00:01<00:08, 43.31ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  23%|██▎       | 104/449 [00:01<00:07, 47.62ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  25%|██▍       | 111/449 [00:01<00:06, 51.19ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  26%|██▋       | 118/449 [00:02<00:06, 54.07ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  28%|██▊       | 125/449 [00:02<00:05, 56.25ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  29%|██▉       | 132/449 [00:02<00:05, 57.92ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  31%|███       | 139/449 [00:02<00:05, 59.18ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  33%|███▎      | 146/449 [00:02<00:05, 59.85ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  34%|███▍      | 153/449 [00:02<00:04, 60.44ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  36%|███▌      | 160/449 [00:02<00:04, 60.60ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  37%|███▋      | 167/449 [00:02<00:04, 61.11ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  39%|███▉      | 174/449 [00:02<00:04, 61.44ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  40%|████      | 181/449 [00:03<00:04, 61.42ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  42%|████▏     | 188/449 [00:03<00:04, 61.53ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  43%|████▎     | 195/449 [00:03<00:04, 61.57ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  45%|████▍     | 202/449 [00:03<00:04, 58.44ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  47%|████▋     | 209/449 [00:03<00:04, 59.24ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  48%|████▊     | 216/449 [00:03<00:03, 59.93ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  50%|████▉     | 223/449 [00:03<00:03, 60.34ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  51%|█████     | 230/449 [00:03<00:03, 60.50ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  53%|█████▎    | 237/449 [00:04<00:03, 60.64ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  54%|█████▍    | 244/449 [00:04<00:03, 59.67ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  56%|█████▌    | 251/449 [00:04<00:03, 60.14ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  57%|█████▋    | 258/449 [00:04<00:03, 60.69ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  59%|█████▉    | 265/449 [00:04<00:03, 59.81ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  61%|██████    | 272/449 [00:04<00:02, 60.30ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  62%|██████▏   | 279/449 [00:04<00:02, 60.19ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  64%|██████▎   | 286/449 [00:04<00:02, 59.88ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  65%|██████▌   | 293/449 [00:04<00:02, 60.34ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  67%|██████▋   | 300/449 [00:05<00:02, 60.57ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  68%|██████▊   | 307/449 [00:05<00:02, 60.30ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  70%|██████▉   | 314/449 [00:05<00:02, 60.64ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  71%|███████▏  | 321/449 [00:05<00:02, 60.66ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  73%|███████▎  | 328/449 [00:05<00:01, 60.53ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  75%|███████▍  | 335/449 [00:05<00:01, 59.84ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  76%|███████▌  | 341/449 [00:05<00:01, 59.13ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  77%|███████▋  | 347/449 [00:05<00:01, 58.35ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  79%|███████▊  | 353/449 [00:05<00:01, 58.25ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  80%|███████▉  | 359/449 [00:06<00:01, 57.87ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  81%|████████▏ | 365/449 [00:06<00:01, 56.82ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  83%|████████▎ | 371/449 [00:06<00:01, 56.92ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  84%|████████▍ | 377/449 [00:06<00:01, 56.95ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  85%|████████▌ | 383/449 [00:06<00:01, 57.00ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  87%|████████▋ | 389/449 [00:06<00:01, 56.03ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  88%|████████▊ | 395/449 [00:06<00:00, 56.96ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  90%|████████▉ | 402/449 [00:06<00:00, 58.35ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  91%|█████████ | 408/449 [00:06<00:00, 57.54ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  92%|█████████▏| 414/449 [00:07<00:00, 57.43ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  94%|█████████▍| 421/449 [00:07<00:00, 58.41ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  95%|█████████▌| 428/449 [00:07<00:00, 59.30ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  97%|█████████▋| 434/449 [00:07<00:00, 58.90ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  98%|█████████▊| 440/449 [00:07<00:00, 58.53ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 449/449 [00:07<00:00, 58.85ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:27<00:00, 27.73s/it]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/39 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  15%|█▌        | 6/39 [00:00<00:00, 58.19ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  31%|███       | 12/39 [00:00<00:00, 58.93ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  46%|████▌     | 18/39 [00:00<00:00, 58.74ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  64%|██████▍   | 25/39 [00:00<00:00, 59.64ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  79%|███████▉  | 31/39 [00:00<00:00, 59.72ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 39/39 [00:00<00:00, 59.90ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  14%|█▍        | 7/50 [00:00<00:00, 62.75ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  28%|██▊       | 14/50 [00:00<00:00, 62.60ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  42%|████▏     | 21/50 [00:00<00:00, 62.18ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  56%|█████▌    | 28/50 [00:00<00:00, 62.45ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  70%|███████   | 35/50 [00:00<00:00, 62.67ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  84%|████████▍ | 42/50 [00:00<00:00, 62.70ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 62.60ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ICKD/yelpreview-raw/commit/5e89bd91c673e1ed971a0360f9ff2c0828da17be', commit_message='Upload dataset', commit_description='', oid='5e89bd91c673e1ed971a0360f9ff2c0828da17be', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset_test = dataset['test'].filter(\n",
    "    lambda batch: len(batch['text']) <= 1000,\n",
    ")\n",
    "filtered_dataset_train = dataset['train'].filter(\n",
    "    lambda batch: len(batch['text']) <= 1000,\n",
    ")\n",
    "filtered_dataset_valid = dataset['valid'].filter(\n",
    "    lambda batch: len(batch['text']) <= 1000,\n",
    ")\n",
    "filtered_dataset = DatasetDict({\n",
    "    'train': filtered_dataset_train,\n",
    "    'test': filtered_dataset_test,\n",
    "    'valid': filtered_dataset_valid\n",
    "})\n",
    "filtered_dataset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d04751-4cd1-45cd-af1f-940bc8bc7bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
